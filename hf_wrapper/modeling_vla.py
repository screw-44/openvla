"""
VLA Policy implementation for LeRobot.

This module implements VLAPolicy which is automatically discovered by LeRobot
through the naming convention: VLAConfig (in configuration_vla.py) â†’ VLAPolicy.

The factory will automatically:
1. Find VLAConfig in PreTrainedConfig.get_known_choices()
2. Infer the module path: configuration_vla â†’ modeling_vla
3. Infer the class name: VLAConfig â†’ VLAPolicy
4. Dynamically import and instantiate VLAPolicy
"""

import json
import logging
from pathlib import Path
from typing import Any, Dict, Optional, Union

import numpy as np
import torch
import torch.nn as nn

from lerobot.policies.pretrained import PreTrainedPolicy
from lerobot.processor import PolicyAction
from transformers import GenerationMixin

from prismatic.models.load import load
from .configuration_vla import VLAConfig
from prismatic.models.vlms.vla import VLA
from prismatic.util.vla_utils import get_vla_tokenizer
from prismatic.vla.tokenizer import BaseTrajectoryConverter, VlaTokenizer
from prismatic.vla.trajectory_compression import BaseTrajectoryCompression
from prismatic.models.vlms.base_vlm import VLM
from prismatic.vla.dataset import DATASET_ITEM_MAP_KEYS

from omegaconf import DictConfig

logger = logging.getLogger(__name__)


class VLAPolicy(PreTrainedPolicy):
    """
    VLA Policy compatible with LeRobot evaluation framework.
    """

    config_class = VLAConfig
    name = "vla"

    def __init__(self, cfg: DictConfig, config: VLAConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.cfg = cfg
        self.model: VLA = None

        self.trajectory_converter: BaseTrajectoryConverter = None
        self.vla_tokenizer: VlaTokenizer = None
        self.trajectory_compression: BaseTrajectoryCompression = None

        self.dataset_name = "HuggingFaceVLA/libero"
        self.decode_control_point = None
        self.reconstruct_traj = None
        self.current_step = 0
        self.last_step = 0
        self.last_action = (0, 0, 0, 0, 0, 0, -1) # åˆå§‹ä½ç½®çš„Action

        self.last_absolute_action = (0, 0, 0, 0, 0, 0, -1) # åˆå§‹ä½ç½®çš„ä½ç½®
        self.abs_reconstruct_traj = None
        self.last_language = None  # ç”¨äºæ£€æµ‹languageæ˜¯å¦å˜åŒ–

    @classmethod
    def from_pretrained(
        cls,
        pretrained_name_or_path: Union[str, Path],
        *,
        config: Optional[VLAConfig] = None,
        **kwargs,
    ) -> "VLAPolicy":
        """
        Load VLA policy from checkpoint.

        Args:
            pretrained_name_or_path: Path to checkpoint directory
            config: Optional VLAConfig (loaded from config.json if not provided)
        """
        pretrained_name_or_path = Path(pretrained_name_or_path)
        import yaml
        config_path = pretrained_name_or_path / "config.yaml"
        with open(config_path) as f:
            cfg = DictConfig(yaml.safe_load(f))

        # Create policy instance
        policy = cls(cfg, config, **kwargs)

        # Load model weights
        policy.model = load(
            vla_cfg=cfg.vla,
            checkpoint_path=pretrained_name_or_path / "model.safetensors",
            load_for_training=False,
        )
        policy.model.to("cuda").eval()  # é»˜è®¤load cuda

        logger.info(f"Loading from the function: load is complete")

        (
            policy.trajectory_converter,
            policy.vla_tokenizer,
            policy.trajectory_compression,
        ) = get_vla_tokenizer(
            trajectory_compression_method=cfg.vla.trajectory.compression_method,
            base_tokenizer=policy.model.llm_backbone.tokenizer,
            prompt_builder_fn=policy.model.llm_backbone.prompt_builder_fn,
            trajectory_converter_type=cfg.vla.trajectory.converter_type,
            trajectory_n_bins=cfg.vla.trajectory.n_bins,
            trajectory_n_dims=cfg.vla.trajectory.n_dims,
        )

        return policy

    # @torch.inference_mode()
    def select_action(self, item: Dict[str, torch.Tensor]) -> torch.Tensor: 
        current_language = item[DATASET_ITEM_MAP_KEYS[self.dataset_name]["language"]][0]
        if self.last_language is None or self.last_language != current_language:
            self.current_step = 0
            self.last_language = current_language
            print(f"ğŸ”„ Language changed, resetting current_step to 0. New language: {current_language}")

        # self.current_step %= 20
        # æ¯10æ­¥ï¼Œè®¡ç®—ä¸€æ¬¡æ¨¡å‹è¾“å‡ºï¼Œè¿›è¡Œçº æ­£ä¸€ä¸‹contorl point
        if self.current_step == 0:
            # ç„¶åå°±å¯ä»¥åˆ©ç”¨vla_tokenzieræ¥è¿›è¡Œbatchçš„å¤„ç†ï¼Œå˜æˆæ¨¡å‹å¯ä»¥è¾“å…¥çš„æ ¼å¼ ï¼ˆimage0æ˜¯è¯„æµ‹çš„å¥‡æ€ªä¸åŒï¼‰
            batch = self.vla_tokenizer.tokenize_input(
                dict(
                    cam1=item[DATASET_ITEM_MAP_KEYS[self.dataset_name]["cam1"]],
                    cam2=item[DATASET_ITEM_MAP_KEYS[self.dataset_name]["cam2"]],
                    language=item[DATASET_ITEM_MAP_KEYS[self.dataset_name]["language"]][0],
                )
            )
            
            # ç»Ÿä¸€æ•°æ®ç±»å‹åˆ°æ¨¡å‹çš„ dtype
            pixel_values, image_transform = (
                batch["pixel_values"],
                self.model.vision_backbone.image_transform,
            )

            pixel_values["cam1"] = (
                image_transform(pixel_values["cam1"]).unsqueeze(0).to(self.model.device)
            )
            pixel_values["cam2"] = (
                image_transform(pixel_values["cam2"]).unsqueeze(0).to(self.model.device)
            )

            input_ids = (
                torch.tensor(batch["prompt_ids"], dtype=torch.long)
                .unsqueeze(0)
                .to(self.model.device)
            )
            print("input_ids:", input_ids)

            with torch.autocast("cuda", dtype=self.model.llm_backbone.half_precision_dtype):
                pred_ids = GenerationMixin.generate(
                    self.model,
                    input_ids=input_ids,
                    pixel_values=pixel_values,
                    use_cache=False,
                    do_sample=False,  # å¼ºåˆ¶è´ªå¿ƒè§£ç 
                    max_new_tokens=1024,  
                )
            pred_ids = pred_ids.cpu().numpy()
            print("shape:", pred_ids.shape, "pred_ids: ", pred_ids)
            input_ids_len = input_ids.shape[1]
            pred_action_ids = pred_ids[0, input_ids_len:] # ä¸€ä¸ªbatchçš„æƒ…å†µ
            
            decoded_control_points = self.trajectory_converter.decode_text_ids_to_trajectory(pred_action_ids)
            # TODOï¼š è¿›è¡Œä¼˜åŒ–ä¿®æ­£control pointï¼Œæ‹¿åˆ°ä¼˜åŒ–è¿‡çš„æ§åˆ¶ç‚¹ã€‚

            # ========================æ£€æµ‹å¹¶åˆ é™¤knotä¸è¿ç»­çš„å¼‚å¸¸ç‚¹ =====
            knot_times = decoded_control_points[:, -1]
            abnormal_indices = []
            
            for i in range(1, len(knot_times)):
                if knot_times[i] < knot_times[i-1]:  # å¦‚æœä¸é€’å¢ï¼ˆå…è®¸knotæ•°å€¼é‡å¤ï¼‰
                    abnormal_indices.append(i)
            
            if abnormal_indices:
                print(f"âš ï¸ æ£€æµ‹åˆ° {len(abnormal_indices)} ä¸ªå¼‚å¸¸control point (knotä¸è¿ç»­)")
                for idx in abnormal_indices:
                    print(f"\nå¼‚å¸¸ä½ç½®: index={idx}")
                    print(f"å‰ä¸€ä¸ªç‚¹ (index={idx-1}): knot_time={knot_times[idx-1]}, data={decoded_control_points[idx-1]}")
                    print(f"å¼‚å¸¸ç‚¹ (index={idx}): knot_time={knot_times[idx]}, data={decoded_control_points[idx]}")
                    if idx + 1 < len(decoded_control_points):
                        print(f"åä¸€ä¸ªç‚¹ (index={idx+1}): knot_time={knot_times[idx+1]}, data={decoded_control_points[idx+1]}")
                
                # åˆ é™¤å¼‚å¸¸ç‚¹
                decoded_control_points = np.delete(decoded_control_points, abnormal_indices, axis=0)
                print(f"\nâœ… å·²åˆ é™¤ {len(abnormal_indices)} ä¸ªå¼‚å¸¸ç‚¹ï¼Œå‰©ä½™ {len(decoded_control_points)} ä¸ªç‚¹")
            else:
                print("âœ“ æ‰€æœ‰control points knotè¿ç»­ï¼Œæ— å¼‚å¸¸ç‚¹")

            # å¦‚æœæ§åˆ¶ç‚¹æ•°é‡å°äº3,å°±ç›´æ¥é‡å¤æœ€åçš„æ§åˆ¶ç‚¹è¡¥è¶³åˆ°3ä¸ª
            if len(decoded_control_points) < 3:
                n_missing = 3 - len(decoded_control_points)
                if len(decoded_control_points) == 0:
                    raise ValueError("decoded_control_points ä¸ºç©ºï¼Œæ— æ³•è¡¥è¶³æ§åˆ¶ç‚¹ï¼")
                last_cp = decoded_control_points[-1:]
                decoded_control_points = np.concatenate([
                    decoded_control_points,
                    np.repeat(last_cp, n_missing, axis=0)
                ], axis=0)
                print(f"âš ï¸ æ§åˆ¶ç‚¹æ•°é‡ä¸è¶³3ï¼Œå·²é‡å¤æœ€åä¸€ä¸ªæ§åˆ¶ç‚¹è¡¥è¶³ï¼Œå½“å‰shape: {decoded_control_points.shape}")
            # =========================é¢å¤–å¼‚å¸¸å¤„ç†ç»“æŸ====================

            if self.decode_control_point is None:
                self.decode_control_point = decoded_control_points
            else: # è¿›è¡Œå¹³æ»‘åŒ–ï¼Œä¸è¿‡ç°åœ¨å°±ç›´æ¥æ›´æ–°æµ‹è¯•ä¸€ä¸‹ï¼Œä¼šä¸ä¼šæœ‰æ”¹å–„
                self.decode_control_point = decoded_control_points

            np.set_printoptions(threshold=np.inf, linewidth=200, suppress=True)
            print("decoded_control_points:",
                  decoded_control_points)

            next_action, bspline = self.trajectory_compression.decode_to_action(
                self.decode_control_point, current_eef_pose=self.last_absolute_action 
            )

            # é‡å»ºæ•´æ¡è½¨è¿¹ï¼šç”¨bsplineåœ¨knotæ—¶é—´ç‚¹ä¸Šé‡‡æ ·
            knot_times = self.decode_control_point[:, -1]
            # é‡‡æ ·æ•°é‡ä¸º knot_times[-1] çš„æ•´æ•°éƒ¨åˆ†ï¼Œä» 0 åˆ° knot_times[-1] -1 ï¼ˆä¸ç®—æœ€åä¸€ä¸ªç‚¹ï¼‰
            self.last_step = int(knot_times[-1]) 
            t_eval = np.arange(0, self.last_step, 1)
            reconstructed_traj = np.zeros((len(t_eval), 7))
            reconstructed_traj[:, :6] = bspline(t_eval)

            self.abs_reconstruct_traj = reconstructed_traj
            # å˜æˆrealtive, NOTEï¼šèµ·å§‹ç‚¹ä¸æ˜¯00000,åªèƒ½ç”¨relativeæ¥æ“ä½œ
            reconstructed_traj[:-1, :6] = np.diff(reconstructed_traj[:, :6], axis=0)
            # gripperé‡‡ç”¨0é˜¶çš„æ’å€¼æ–¹å¼
            indices = np.searchsorted(knot_times, t_eval, side='right') - 1
            indices = np.clip(indices, 0, len(decoded_control_points) - 1)  # ä¿®æ­£gripper bug
            reconstructed_traj[:, 6] = self.decode_control_point[indices, 6]
            
            # æ·»åŠ ä¸€ä¸ªç»“å°¾åœæ­¢çš„åŠ¨ä½œ
            reconstructed_traj = np.vstack([reconstructed_traj, np.array([[0, 0, 0, 0, 0, 0, -1]])])
            self.reconstruct_traj = reconstructed_traj
            print("reconstructed traj:", self.reconstruct_traj)

        # æ›´æ–°ä¸€ä¸‹last absolute actionç”¨æ¥ä¸‹æ¬¡è¿›è¡Œeef poseçš„æ›´æ–°
        # self.last_absolute_action = self.abs_reconstruct_traj[self.current_step]
        action = self.reconstruct_traj[self.current_step]
        print("self.current_step:", self.current_step, " .conduct action:", action)

        self.current_step += 1
        self.current_step = min(self.current_step, self.last_step) # å¦‚æœé¢„æµ‹çš„è½¨è¿¹æ— æ³•æ‰§è¡ŒæˆåŠŸï¼Œå°±ä¿æŒä¸åŠ¨
        self.last_action = action
        return torch.Tensor(action).unsqueeze(0)

    def predict_action_chunk(
        self,
        observation: Dict[str, Union[torch.Tensor, np.ndarray]],
        **kwargs,
    ) -> torch.Tensor:
        """
        Predict action chunk (sequence of actions).
        NOTE: action horizonè¿™ä¸ªä¸œè¥¿è®©æ¨¡å‹è‡ªå·±å»å¤„ç†
        Used by action-chunking policies to predict multiple future steps.

        Args:
            observation: Dict with "image" and "task"

        Returns:
            torch.Tensor of shape [B, action_horizon, action_dim]
        """
        print("è¿›å…¥predict action chunkçš„åœ°æ–¹")
        # For VLA, we typically generate one step at a time
        # But we can repeat to create a chunk
        action = self.select_action(observation, **kwargs)

        return action

    def forward(self, batch: Dict[str, torch.Tensor]):
        raise NotImplementedError

    def reset(self) -> None:
        pass

    def get_optim_params(self) -> list:
        pass
    # ============ Standard nn.Module methods ============

    def to(self, device: Union[str, torch.device], **kwargs) -> "VLAPolicy":
        """Move policy to device."""
        self.model = self.model.to(device, **kwargs)
        return self

    def eval(self) -> "VLAPolicy":
        """Set to evaluation mode."""
        self.model.eval()
        return self

    def train(self, mode: bool = True) -> "VLAPolicy":
        """Set to training mode."""
        self.model.train(mode)
        return self

    @property
    def device(self) -> torch.device:
        """Get device of model parameters."""
        return next(self.model.parameters()).device


if __name__ == "__main__":
    # è¿™é‡Œæ˜¯å¦‚æœæ˜¯ä¸€ä¸ªæµ‹è¯•ï¼Œç”¨æ¥ä»lerobotæ•°æ®é›†è¯»å–çš„å›¾åƒï¼Œæ¥ç»™è¿™ä¸ªpolicyè¿›è¡Œæµ‹è¯•ï¼Œçœ‹è¾“å‡ºæ˜¯å¦æ­£å¸¸ã€‚
    # ä»¥åŠå¯ä»¥å¯¹æ¯”ä¸€ä¸‹å¦‚æœç”¨è®­ç»ƒçš„æ–¹å¼ï¼Œæ˜¯å¦æ˜¯æ­£å¸¸çš„ã€‚
    policy = VLAPolicy.from_pretrained(
        pretrained_name_or_path="/inspire/hdd/project/robot-decision/hexinyu-253108100063/Project/Aff/vla_runs/"
        + "base+b64+x7--2_train2eval_without_flashattn/"
    )
    from lerobot.datasets.lerobot_dataset import LeRobotDataset

    dataset = LeRobotDataset(
        policy.dataset_name,
        root=Path(
            f"/inspire/hdd/project/robot-decision/public/datasets/{policy.dataset_name}"
        ),
    )

    from torch.utils.data import DataLoader

    base_tokenizer = policy.model.llm_backbone.tokenizer
    # from prismatic.util.data_utils import PaddedCollatorForActionPrediction
    # collator = PaddedCollatorForActionPrediction(
    #     base_tokenizer.model_max_length, base_tokenizer.pad_token_id
    # )
    dataloader = DataLoader(
        dataset, batch_size=1, shuffle=False
    )  # , collate_fn=collator)

    for idx, batch in enumerate(dataloader):
        print(f"===== Sample {idx} =====")
        action = policy.select_action(batch)
        print("policyæµ‹è¯•è¾“å‡ºaction:", action)

        break
