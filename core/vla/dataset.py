"""
ç”¨lerobot3.0æ•°æ®é›†æ ¼å¼ï¼Œé«˜æ•ˆç‡çš„å®ç°datasetçš„è¯»å–ã€‚

ä½¿ç”¨LeRobotDatasetMetadataå…ˆè¿‡æ»¤taskï¼Œç„¶åç”¨LeRobotDatasetåŠ è½½æŒ‡å®šçš„episodesã€‚

æ ¸å¿ƒåŠŸèƒ½:
1. æ”¯æŒæŒ‰task_idsè¿‡æ»¤episodes
2. æ”¯æŒé™åˆ¶æ¯ä¸ªtaskåŠ è½½çš„episodeæ•°é‡
3. ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ future_actionsï¼ˆä»å½“å‰åˆ°episodeç»“æŸçš„æ‰€æœ‰actionsï¼‰
4. å¯é…ç½®çš„å¤„ç†é¢‘ç‡(process_hz)å’Œbatchå˜æ¢
"""
import torch
import json
import numpy as np
import random

from time import time
from pathlib import Path
from typing import Tuple

from torch.utils.data import DataLoader
from core.models.backbones.llm.prompting import PurePromptBuilder
from core.models.backbones.vision.base_vision import ImageTransform
from lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata
from core.vla.trajectory_compression import BaseTrajectoryCompression, BiningTrajectoryCompression
from core.vla.tokenizer import VlaTokenizer, BaseTrajectoryConverter
from core.util.overwatch import initialize_overwatch

# ä¸åŒçš„Datasetæœ‰ä¸åŒçš„keyæ˜ å°„ï¼Œuniform_key
DATASET_ITEM_MAP_KEYS ={
    'HuggingFaceVLA/libero': {
        'cam1': 'observation.images.image', # è¿˜æœ‰ observation.images.image2 (ä¸¤ä¸ªcamera)
        'cam2': 'observation.images.image2',
        'language': 'task',
    },
}

class MyLeRobotDataset(torch.utils.data.Dataset):
    def __init__(
            self, 
            repo_id: str, 
            image_transform: ImageTransform,
            tokenizer: VlaTokenizer,
            trajectory_compression: BaseTrajectoryCompression,
            real_root:Path=Path("/inspire/hdd/project/robot-decision/public/datasets/"), 
            **kwargs
        ):
        self.repo_id = repo_id
        self.tokenizer = tokenizer 
        self.traj_compress = trajectory_compression
        self.root = real_root / repo_id
        # NOTE: å®Œå…¨åˆ é™¤æ‰metadataè¿™ä¸ªç±»ï¼Œç›´æ¥ç¦»çº¿å¤„ç†ï¼Œæ‹¿åˆ°episode indexç›´æ¥åœ¨get itemä¸­è¿‡æ»¤ã€‚

        self.overwatch = initialize_overwatch(__name__)
        self._dataset = LeRobotDataset(
            repo_id,
            root=self.root,
            episodes=None, 
            image_transforms=image_transform,
        ) # NOTEï¼šä¸éœ€è¦é‡‡ç”¨ä¸“é—¨çš„delta_timestampsäº†ï¼Œæˆ‘ä»¬æ˜¯ä»ç¦»çº¿è·å–çš„ï¼Œæ‰€ä»¥ç®€åŒ–ä»£ç äº†è¿™é‡Œ
        self.overwatch.info(f"training dataset length:{len(self._dataset)}") #, validate dataset length:{len(self.val_dataset)}")

    @property
    def dataset(self): return self._dataset 

    def __len__(self): return len(self.dataset)
        
    def __getitem__(self, index):
        # æ ¹æ®æ˜¯å“ªä¸€ä¸ªå…·ä½“çš„æ•°æ®é›†ï¼Œæ‹¿åˆ°å¯¹åº”çš„æ•°æ®
        item = self.dataset.__getitem__(index)
        frame_index, episode_index = item['frame_index'], item['episode_index']

        # è¿™é‡Œæ‰©å±•åˆ°äº†ä¸¤å›¾è¾“å…¥çš„liberoçš„æ ¼å¼ï¼ˆç›®å‰å…ˆfocusåœ¨liberoä¸Šï¼‰
        uni_key_item = dict(
            cam1=item[DATASET_ITEM_MAP_KEYS[self.repo_id]['cam1']],
            cam2=item[DATASET_ITEM_MAP_KEYS[self.repo_id]['cam2']],
            language=item[DATASET_ITEM_MAP_KEYS[self.repo_id]['language']],
            trajectory=self.traj_compress(frame_index, episode_index),
            state=item['observation.state'],
            dataset_names=self.repo_id
        )

        return self.tokenizer.tokenize_batch(uni_key_item)
    

if __name__ == "__main__":
    def _test_my_dataset_full():
            """
            ä¸‰ç§æ¨¡å¼ï¼š
            - MODE="dataset": åªæµ‹ GT labels -> decode -> reconstructï¼ˆä¿æŒä½ åŸæœ‰åŠŸèƒ½/å¯è§†åŒ–é£æ ¼ï¼‰
            - MODE="model"  : åªæµ‹ model generate -> decode -> reconstruct
            - MODE="both"   : ä¸¤è€…éƒ½æµ‹ + æ‰“å° GT vs Pred å¯¹æ¯”ï¼ˆå¯è§†åŒ–ä»å„è‡ªå•ç‹¬ä¿å­˜ï¼Œé£æ ¼ä¸å˜ï¼‰
            """
            from pathlib import Path
            import numpy as np
            from tqdm import tqdm

            import matplotlib
            matplotlib.use("Agg")
            import matplotlib.pyplot as plt

            # ==================== é…ç½®å‚æ•°ï¼ˆå†™æ­»ï¼Œæœ€ç®€å•ï¼‰ ====================
            MODE = "both"  # "dataset" | "model" | "both"
            NEED_DATASET = MODE in ("dataset", "both")
            NEED_MODEL = MODE in ("model", "both")

            # é‡‡æ ·ç­–ç•¥ï¼ˆä¿æŒä½ ä¹‹å‰é»˜è®¤ï¼šä»9900å¼€å§‹ï¼Œæ¯100ä¸ªå–ä¸€ä¸ªï¼‰
            START_IDX = 0
            STRIDE = 300

            # æ˜¯å¦æ¯ä¸ªæ ·æœ¬æš‚åœï¼ˆä½ ä¹‹å‰æ˜¯å¿…æš‚åœï¼›è¿™é‡Œé»˜è®¤ä¿æŒä¸€è‡´ï¼‰
            PAUSE_EACH_SAMPLE = True

            # model æ¨ç†å‚æ•°
            MAX_NEW_TOKENS = 10240

            config_dict = {
                "repo_id": "HuggingFaceVLA/libero",
                "compression_method": "bspline_v3",
                "converter_type": "bspline_v3",
                "dataset_root": Path("/inspire/hdd/project/robot-decision/public/datasets/HuggingFaceVLA/libero"),
            }

            # ä½ çš„è®­ç»ƒè¾“å‡ºç›®å½•ï¼ˆç”¨äºåŠ è½½ vla æ¨¡å‹ï¼‰
            config_path = "/inspire/ssd/project/robot-decision/hexinyu-253108100063/Project/Aff/vla/outputs/" \
            "2026-01-13/04-48-57/qwen2.5-0.5b+b16+x7--1-qwen25-abs_aff_uniform_bspline_v3"
            ckpt_path = Path(config_path) / "checkpoints" / "step-015000-epoch-00-loss=0.1329.safetensors" # "latest-checkpoint.safetensors" # "step-035000-epoch-02-loss=0.0147.safetensors" # "step-085000-epoch-04-loss=0.0517.safetensors"

            print("=" * 80)
            print("ã€æ•°æ®é›†/æ¨¡å‹ ç¼–ç -è§£ç æµ‹è¯•ã€‘")
            print("=" * 80)
            print(f"MODE: {MODE}")
            print(f"é…ç½®: {config_dict}\n")

            # ==================== åŠ è½½æ•°æ®é›†ï¼ˆrawï¼‰ ====================
            print("ğŸ“– æ­£åœ¨åŠ è½½å®Œæ•´æ•°æ®é›†...")

            full_traj_dataset = LeRobotDataset(
                config_dict["repo_id"],
                root=config_dict["dataset_root"],
                delta_timestamps={"abs_aff": []},
            )
            print(f"âœ“ å®Œæ•´æ•°æ®é›†å¤§å°: {len(full_traj_dataset)}")

            # ==================== åŠ è½½ VLA æ•°æ®é›† + tokenizer / transformï¼ˆdataset-only é»˜è®¤ï¼‰ ====================
            print("\nğŸ“Š æ­£åœ¨åŠ è½½ VLA æ•°æ®é›†...")
            from transformers import AutoTokenizer
            from core.util.vla_utils import get_vla_dataset

            # dataset-only çš„é»˜è®¤ tokenizer / promptï¼ˆä¿æŒä½ åŸé€»è¾‘ï¼‰
            base_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")

            prompt_builder_fn = PurePromptBuilder
            image_transform = None

            # ==================== å¯é€‰ï¼šåŠ è½½æ¨¡å‹å¹¶å¯¹é½ tokenizer / prompt / transform ====================
            vla = None
            if NEED_MODEL:
                import torch
                from omegaconf import OmegaConf
                from core.models.load import load

                run_dir = Path(config_path)

                # Hydra è¾“å‡ºä¸€èˆ¬åœ¨ .hydra/config.yamlï¼›æœ‰äº›å·¥ç¨‹ä¹Ÿä¼šæ”¾åœ¨æ ¹ç›®å½• config.yaml
                cfg_path_candidates = [
                    run_dir / ".hydra" / "config.yaml",
                    run_dir / "config.yaml",
                ]
                cfg_path = None
                for p in cfg_path_candidates:
                    if p.exists():
                        cfg_path = p
                        break
                if cfg_path is None:
                    raise FileNotFoundError(f"æ‰¾ä¸åˆ° config.yamlï¼ˆå·²å°è¯•: {cfg_path_candidates}ï¼‰")

                cfg = OmegaConf.load(cfg_path)

                vla = load(
                    vla_cfg=cfg.vla,
                    checkpoint_path=ckpt_path,
                    load_for_training=False,
                )
                vla = vla.to(device="cuda", dtype=torch.bfloat16).eval()

                # å…³é”®ï¼šæ¨ç†å¿…é¡»ä¸æ¨¡å‹ tokenizer / prompt / image_transform å¯¹é½
                base_tokenizer = vla.llm_backbone.get_tokenizer()
                prompt_builder_fn = vla.llm_backbone.prompt_builder_fn
                image_transform = vla.vision_backbone.get_image_transform()

            vla_dataset, trajectory_converter, collator = get_vla_dataset(
                data_repo_id=config_dict["repo_id"],
                data_task_ids=None,
                trajectory_compression_method=config_dict["compression_method"],
                trajectory_converter_type=config_dict["converter_type"],
                base_tokenizer=base_tokenizer,
                prompt_builder_fn=prompt_builder_fn,
                image_transform=image_transform,
            )
            print(f"âœ“ VLA æ•°æ®é›†å¤§å°: {len(vla_dataset)}")

            # ==================== å·¥å…·å‡½æ•°ï¼šæ‹†åˆ† prompt / action token ====================
            def split_prompt_and_action(labels_np: np.ndarray):
                if (labels_np == -100).any():
                    prompt_len = int(np.where(labels_np == -100)[0][-1] + 1)
                    return prompt_len, labels_np[prompt_len:]
                return 0, labels_np

            # ==================== decode -> reconstruct -> errorï¼ˆä¸æ”¹å˜ä½ æ ¸å¿ƒé€»è¾‘ï¼‰ ====================
            def decode_and_reconstruct(token_ids: np.ndarray, abs_aff_gt: np.ndarray, frame_index: int):
                decoded_cp = trajectory_converter.decode_text_ids_to_trajectory(token_ids)

                # ä½ åŸæ¥å°±æ˜¯è¿™æ ·æ‹¿ä¸¤ä¸ª bspline
                bspline, gripper_bspline = vla_dataset.traj_compress.decode_to_action(decoded_cp)

                knots = decoded_cp[:, -1]
                num_samples = knots[-1].astype(int) + 1  # âœ… æŒ‰ä½ è¦æ±‚ä¿ç•™
                t_eval = np.arange(num_samples)

                reconstructed = np.zeros((num_samples, 7))
                reconstructed[:, :6] = bspline(t_eval)

                # âœ… æŒ‰ä½ è¦æ±‚ï¼šåˆ é™¤ random æ‰°åŠ¨ï¼›ä½¿ç”¨çœŸå® current poseï¼ˆå‰6ç»´ï¼‰
                current_pose = abs_aff_gt[frame_index][:6]
                print("current_pose is:", current_pose)
                print("bspline 0 pose: ", bspline(0))

                if False and num_samples > 20:
                    offset = vla_dataset.traj_compress.start_offset(current_pose, bspline(10))
                    print("offset:", offset)
                    L = offset.shape[0]
                    reconstructed[:L, :6] += offset

                reconstructed[:, 6] = gripper_bspline(t_eval)

                gt_segment = abs_aff_gt[frame_index : frame_index + num_samples]
                if len(reconstructed) != len(gt_segment):
                    print("âš ï¸ é‡å»ºé•¿åº¦ä¸GTé•¿åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œè£å‰ªå¯¹é½")
                    min_len = min(len(reconstructed), len(gt_segment))
                    errors = np.abs(reconstructed[:min_len] - gt_segment[:min_len])
                else:
                    errors = np.abs(reconstructed - gt_segment)

                return decoded_cp, reconstructed, gt_segment, errors, float(np.mean(errors)), float(np.max(errors)), float(np.std(errors))

            # ==================== å¯è§†åŒ–ï¼ˆä¿æŒä½ ä¹‹å‰ä»£ç é£æ ¼ä¸€è‡´ï¼‰ ====================
            def plot_like_before(decoded_cp, reconstructed, gt_segment, frame_index, episode_index, idx, stats_list, output_path: Path, title_prefix: str | None):
                # æ—¶é—´è½´
                gt_t = np.arange(len(gt_segment)) + frame_index
                knots_vis = decoded_cp[:, -1] + frame_index
                t_eval_vis = np.arange(len(reconstructed)) + frame_index

                fig, axes = plt.subplots(4, 2, figsize=(16, 14))
                axes = axes.flatten()
                dims_to_plot = [0, 1, 2, 3, 4, 5]
                dim_names = ["x", "y", "z", "yaw", "pitch", "roll"]

                for i, (dim, dim_name) in enumerate(zip(dims_to_plot, dim_names)):
                    ax = axes[i]
                    ax.plot(gt_t, gt_segment[:, dim], label=f"GT {dim_name}", linewidth=2, alpha=0.8, color="green")
                    ax.plot(t_eval_vis, reconstructed[:, dim], label=f"Reconstructed {dim_name}",
                            linestyle="--", linewidth=1.5, alpha=0.8, color="red")
                    ax.scatter(knots_vis, decoded_cp[:, dim], c="red", s=50, marker="x", label="Control Points", zorder=5)
                    ax.set_ylabel(dim_name)
                    ax.set_xlabel("Time (frames)")
                    ax.set_title(f"Dimension: {dim_name}")
                    ax.legend(loc="upper right")
                    ax.grid(True, alpha=0.3)

                # gripper
                axes[6].plot(gt_t, gt_segment[:, 6], label="GT gripper", linewidth=2, alpha=0.8, color="green")
                axes[6].plot(t_eval_vis, reconstructed[:, 6], label="Reconstructed gripper",
                            linestyle="--", linewidth=1.5, alpha=0.8, color="red")
                axes[6].scatter(knots_vis, decoded_cp[:, 6], c="red", s=50, marker="x", label="Control Points", zorder=5)
                axes[6].set_ylabel("gripper")
                axes[6].set_xlabel("Time (frames)")
                axes[6].set_title("Dimension: gripper")
                axes[6].legend(loc="upper right")
                axes[6].grid(True, alpha=0.3)

                # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿æŒä½ ä¹‹å‰â€œç´¯è®¡ç»Ÿè®¡â€çš„æ˜¾ç¤ºæ–¹å¼ï¼‰
                mean_errs = np.array([s["mean_error"] for s in stats_list], dtype=np.float64)
                max_errs = np.array([s["max_error"] for s in stats_list], dtype=np.float64)
                std_errs = np.array([s["std_error"] for s in stats_list], dtype=np.float64)

                axes[7].axis("off")
                stats_text = f"""Error Statistics:
        Mean Error:  {np.mean(mean_errs):.6f} +- {np.std(mean_errs):.6f}
        Max Error:   {np.mean(max_errs):.6f} +- {np.std(max_errs):.6f}
        Std Error:   {np.mean(std_errs):.6f} +- {np.std(std_errs):.6f}
        Visualized Sample:
        Control_points: {len(decoded_cp)}
        Original Length: {len(gt_segment)}
        Compression Ratio: {len(gt_segment)/len(decoded_cp):.2f}
        Episode: {episode_index}
        Frame: {frame_index}
        Index: {idx}
    """
                axes[7].text(0.1, 0.5, stats_text, fontsize=10,
                            verticalalignment="center",
                            bbox=dict(boxstyle="round", facecolor="lightblue", alpha=0.5))
                axes[7].set_visible(True)

                title = f"Trajectory Reconstruction (GT vs Reconstructed) - Ep{episode_index}, Frame{frame_index}"
                if title_prefix:
                    title = f"{title_prefix} | " + title

                plt.suptitle(title, fontsize=14, fontweight="bold")
                plt.tight_layout()
                plt.savefig(output_path, dpi=150, bbox_inches="tight")
                plt.close()

            # ==================== éå†æ‰€æœ‰æ ·æœ¬è¿›è¡Œæµ‹è¯• ====================
            print("ğŸ”„ æ­£åœ¨éå†å¹¶æµ‹è¯•æ‰€æœ‰æ ·æœ¬...\n")

            gt_error_stats = []
            pred_error_stats = []

            sample_count = 0
            gt_success = 0
            pred_success = 0
            skip_count = 0

            # ï¼ˆä½ ä¹‹å‰æœ‰ filter_index é€»è¾‘ï¼Œè¿™é‡Œä¿æŒæ³¨é‡Šï¼Œä¸å½±å“ï¼‰
            # filter_index = vla_dataset.filter_index

            for idx in tqdm(range(START_IDX, len(full_traj_dataset), STRIDE), desc="å¤„ç†ä¸­"):
                raw_item = full_traj_dataset[idx]
                frame_index = int(raw_item["frame_index"])
                episode_index = int(raw_item["episode_index"])

                # if episode_index in filter_index:
                #     skip_count += 1
                #     print("skipping episode index:", episode_index)
                #     continue

                sample_count += 1

                # raw è½¨è¿¹ï¼ˆå·®åˆ† -> ç»å¯¹ï¼‰
                abs_aff = raw_item["abs_aff"].numpy()
                abs_aff_gt = abs_aff.copy()
                abs_aff_gt[:, :-1] = np.cumsum(abs_aff_gt[:, :-1], axis=0)

                vla_item = vla_dataset[idx]
                labels = vla_item["labels"].numpy()
                prompt_len, gt_action_ids = split_prompt_and_action(labels)

                gt_res = None
                pred_res = None

                # ========= 1) GT(dataset) è¯„ä¼°ï¼ˆä¿æŒä½ åŸé€»è¾‘/å¯è§†åŒ–ï¼‰ =========
                if NEED_DATASET:
                    if len(gt_action_ids) < 8:
                        print(f"[GT] skip: action token too short. idx={idx}")
                    else:
                        decoded_cp, reconstructed, gt_segment, errors, mean_e, max_e, std_e = decode_and_reconstruct(
                            gt_action_ids, abs_aff_gt, frame_index
                        )
                        gt_success += 1
                        gt_error_stats.append({
                            "sample_idx": idx,
                            "episode_idx": episode_index,
                            "frame_idx": frame_index,
                            "mean_error": mean_e,
                            "max_error": max_e,
                            "std_error": std_e,
                        })

                        # ä½ çš„è¾“å‡ºè·¯å¾„ä¿æŒä¸å˜
                        out_path = Path("/tmp/dataset_decode_error_analysis.png")
                        plot_like_before(
                            decoded_cp=decoded_cp,
                            reconstructed=reconstructed,
                            gt_segment=gt_segment,
                            frame_index=frame_index,
                            episode_index=episode_index,
                            idx=idx,
                            stats_list=gt_error_stats,
                            output_path=out_path,
                            title_prefix=None,  # ä¿æŒä½ ä¹‹å‰æ ‡é¢˜æ ¼å¼
                        )
                        print(f"âœ“ [GT] å¯è§†åŒ–å·²ä¿å­˜: {out_path}")
                        gt_res = (mean_e, max_e, std_e, decoded_cp, reconstructed, gt_segment)

                # ========= 2) Model è¯„ä¼° =========
                if NEED_MODEL:
                    import torch
                    # print("full input ids:", vla_item["input_ids"].numpy())
                    # prompt è¾“å…¥ï¼šåªå–‚ prompt éƒ¨åˆ†ï¼ˆå’Œä½ ä¹‹å‰ä¸€è‡´ï¼‰
                    input_ids = vla_item["input_ids"][:prompt_len].unsqueeze(0).to("cuda")
                    attn = vla_item.get("attention_mask", None)
                    if attn is not None:
                        attn = attn[:prompt_len].unsqueeze(0).to("cuda")

                    pixel_values = vla_item.get("pixel_values", None)
                    if isinstance(pixel_values, dict):
                        pixel_values = {k: v.unsqueeze(0).to("cuda") for k, v in pixel_values.items()}
                    elif pixel_values is not None:
                        pixel_values = pixel_values.unsqueeze(0).to("cuda")

                    gen_kwargs = dict(
                        input_ids=input_ids,
                        use_cache=False,
                        do_sample=False,
                        max_new_tokens=MAX_NEW_TOKENS,
                    )
                    if attn is not None:
                        gen_kwargs["attention_mask"] = attn
                    if pixel_values is not None:
                        gen_kwargs["pixel_values"] = pixel_values

                    half_dtype = getattr(vla.llm_backbone, "half_precision_dtype", torch.bfloat16)

                    with torch.inference_mode(), torch.autocast("cuda", dtype=half_dtype):
                        from transformers.generation.utils import GenerationMixin
                        pred_ids = GenerationMixin.generate(vla, **gen_kwargs)

                    pred_ids = pred_ids[0].detach().cpu().numpy()
                    # print("model pred_ids:", pred_ids)
                    pred_action_ids = pred_ids[prompt_len:]

                    if pred_action_ids is None or len(pred_action_ids) < 8:
                        print(f"[Pred] skip: action token too short after sanitize. idx={idx}, len={0 if pred_action_ids is None else len(pred_action_ids)}")
                    else:
                        decoded_cp, reconstructed, gt_segment, errors, mean_e, max_e, std_e = decode_and_reconstruct(
                            pred_action_ids, abs_aff_gt, frame_index
                        )
                        pred_success += 1
                        pred_error_stats.append({
                            "sample_idx": idx,
                            "episode_idx": episode_index,
                            "frame_idx": frame_index,
                            "mean_error": mean_e,
                            "max_error": max_e,
                            "std_error": std_e,
                        })

                        out_path = Path("/tmp/model_decode_error_analysis.png")
                        plot_like_before(
                            decoded_cp=decoded_cp,
                            reconstructed=reconstructed,
                            gt_segment=gt_segment,
                            frame_index=frame_index,
                            episode_index=episode_index,
                            idx=idx,
                            stats_list=pred_error_stats,
                            output_path=out_path,
                            title_prefix="Pred",  # åªåœ¨æ¨¡å‹å›¾ä¸ŠåŠ å‰ç¼€ï¼Œä¸å½±å“ä½ åŸ GT å›¾
                        )
                        print(f"âœ“ [Pred] å¯è§†åŒ–å·²ä¿å­˜: {out_path}")
                        pred_res = (mean_e, max_e, std_e, decoded_cp, reconstructed, gt_segment, pred_action_ids)

                # ========= 3) both æ¨¡å¼ä¸‹ï¼šæ‰“å° GT vs Pred å·®è·ï¼ˆä¸æ”¹ä½ å¯è§†åŒ–ï¼‰ =========
                if MODE == "both" and (gt_res is not None) and (pred_res is not None):
                    gt_mean, gt_max, gt_std, _, _, _ = gt_res
                    pr_mean, pr_max, pr_std, _, _, _, pred_action_ids = pred_res

                    m = min(len(gt_action_ids), len(pred_action_ids))
                    match = float((gt_action_ids[:m] == pred_action_ids[:m]).mean()) if m > 0 else 0.0

                    print(f"[GT vs Pred] token match (prefix {m}): {match*100:.2f}% | GT_len={len(gt_action_ids)} Pred_len={len(pred_action_ids)}")
                    print(f"[GT recon]   mean={gt_mean:.6f} max={gt_max:.6f} std={gt_std:.6f}")
                    print(f"[Pred err]   mean={pr_mean:.6f} max={pr_max:.6f} std={pr_std:.6f}")

                if PAUSE_EACH_SAMPLE and (NEED_DATASET or NEED_MODEL):
                    input("æŒ‰å›è½¦é”®ç»§ç»­ä¸‹ä¸€ä¸ªæ ·æœ¬...")

            # ==================== æ‰“å°ç»Ÿè®¡ç»“æœ ====================
            print("\n" + "=" * 80)
            print("ã€ç»Ÿè®¡ç»“æœã€‘")
            print("=" * 80)
            print(f"æ€»å¤„ç†æ ·æœ¬æ•°: {sample_count}")
            print(f"è·³è¿‡çš„ episode: {skip_count}")

            if NEED_DATASET and len(gt_error_stats) > 0:
                mean_errs = np.array([s["mean_error"] for s in gt_error_stats])
                max_errs = np.array([s["max_error"] for s in gt_error_stats])
                std_errs = np.array([s["std_error"] for s in gt_error_stats])
                print("\nã€GT(dataset) è¯¯å·®ç»Ÿè®¡ã€‘")
                print(f"  Mean Error: {np.mean(mean_errs):.6f} Â± {np.std(mean_errs):.6f}")
                print(f"  Max Error:  {np.mean(max_errs):.6f} Â± {np.std(max_errs):.6f}")
                print(f"  Std Error:  {np.mean(std_errs):.6f} Â± {np.std(std_errs):.6f}")
                print(f"  æˆåŠŸæ ·æœ¬æ•°: {gt_success}")

            if NEED_MODEL and len(pred_error_stats) > 0:
                mean_errs = np.array([s["mean_error"] for s in pred_error_stats])
                max_errs = np.array([s["max_error"] for s in pred_error_stats])
                std_errs = np.array([s["std_error"] for s in pred_error_stats])
                print("\nã€Model(Pred) è¯¯å·®ç»Ÿè®¡ã€‘")
                print(f"  Mean Error: {np.mean(mean_errs):.6f} Â± {np.std(mean_errs):.6f}")
                print(f"  Max Error:  {np.mean(max_errs):.6f} Â± {np.std(max_errs):.6f}")
                print(f"  Std Error:  {np.mean(std_errs):.6f} Â± {np.std(std_errs):.6f}")
                print(f"  æˆåŠŸæ ·æœ¬æ•°: {pred_success}")

            print("\nâœ… æµ‹è¯•å®Œæˆï¼")
            return gt_error_stats, pred_error_stats

    _test_my_dataset_full()

