"""
qwen3_vla.py

å®Œå…¨ç‹¬ç«‹çš„ Qwen3-VL æ¨¡å‹å®ç°ï¼Œç”¨äº VLA ä»»åŠ¡ã€‚
ä¸ä¾èµ– prismatic çš„ vision backbone / llm backbone åˆ†ç¦»æ¶æ„ã€‚

å…³é”®è®¾è®¡ï¼š
- Qwen3-VL æ˜¯ä¸€ä½“å¼æ¨¡å‹ï¼ˆvision + language èåˆï¼‰
- ä½¿ç”¨ processor-driven tokenization å¤„ç†å›¾åƒå’Œæ–‡æœ¬
- æ”¯æŒåŒæ‘„åƒå¤´è¾“å…¥
- åŠ¨ä½œé€šè¿‡ VlaTokenizer ç¦»æ•£åŒ–åè¿½åŠ åˆ°åºåˆ—æœ«å°¾
- æ”¯æŒ left padding
- å¯é€‰æ‹©æ€§å†»ç»“ vision/language éƒ¨åˆ†
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Callable, Dict, List, Optional, Union

import numpy as np
import torch
import torch.nn as nn
from PIL import Image
from transformers import (
    AutoProcessor,
    GenerationMixin,
    PretrainedConfig,
)
from transformers.models.qwen3_vl.modeling_qwen3_vl import (
    Qwen3VLForConditionalGeneration,
)
from transformers.modeling_outputs import CausalLMOutputWithPast

from core.models.backbones.llm.prompting import PromptBuilder
from core.models.vlms.base_vlm import VLM
from core.util.overwatch import initialize_overwatch
from core.vla.tokenizer import BaseTrajectoryConverter

# Initialize Overwatch =>> Wraps `logging.Logger`
overwatch = initialize_overwatch(__name__)

# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)
IGNORE_INDEX = -100


class Qwen3VLA(VLM):
    """
    Qwen3-VL based VLA model.

    ä¸ PrismaticVLM ä¸åŒï¼Œè¿™æ˜¯ä¸€ä½“å¼æ¨¡å‹ï¼š
    - ä¸åˆ†ç¦» vision_backbone / llm_backbone / projector
    - ä½¿ç”¨ Qwen3 è‡ªå·±çš„ vision encoder å’Œ multimodal fusion
    - forward() ç›´æ¥å¤„ç†å›¾åƒå’Œæ–‡æœ¬ï¼Œä¸éœ€è¦é¢„å…ˆæå– vision features
    """

    def __init__(
        self,
        model_id: str,
        model_size: str = "2B",  # "2B", "4B", "7B"
        trajectory_converter: Optional[BaseTrajectoryConverter] = None,
        enable_mixed_precision_training: bool = True,
        hf_cache_dir: Optional[Path] = None,
        **kwargs,
    ) -> None:
        # æ³¨æ„ï¼šæˆ‘ä»¬ä¸è°ƒç”¨ super().__init__ï¼Œå› ä¸ºåŸºç±»æœŸæœ› vision_backbone/llm_backbone
        # ç›´æ¥ç»§æ‰¿ nn.Module å’Œ GenerationMixin
        nn.Module.__init__(self)
        GenerationMixin.__init__(self)

        self.model_family = "qwen3-vl"
        self.model_id = model_id
        self.model_size = model_size
        self.enable_mixed_precision_training = enable_mixed_precision_training
        self.trajectory_converter = trajectory_converter

        # æ„å»º HF model path (Qwen3-VL)
        size_to_hub = {
            "2B": "Qwen/Qwen3-VL-2B-Instruct",
            "4B": "Qwen/Qwen3-VL-4B-Instruct",
            "7B": "Qwen/Qwen3-VL-7B-Instruct",
        }
        self.hf_hub_path = size_to_hub.get(model_size, size_to_hub["2B"])

        # Resolve HF cache path
        if hf_cache_dir is None:
            hf_home = os.environ.get("HF_HOME", Path.home() / ".cache" / "huggingface")
            hf_cache_dir = Path(hf_home) / "hub"
        self.hf_cache_dir = hf_cache_dir

        overwatch.info(
            f"Loading Qwen3-VL {model_size} from HF cache (offline mode)",
            ctx_level=1,
        )

        # Load processor (handles image preprocessing + tokenization)
        # processor éœ€è¦ trust_remote_code=True
        try:
            self.processor = AutoProcessor.from_pretrained(
                self.hf_hub_path,
                trust_remote_code=True,
                local_files_only=True,
                cache_dir=self.hf_cache_dir,
            )
            overwatch.info(f"Processor loaded from local cache", ctx_level=1)
        except Exception as e:
            overwatch.warning(
                f"Failed to load processor from local cache: {e}. Attempting online download..."
            )
            self.processor = AutoProcessor.from_pretrained(
                self.hf_hub_path,
                trust_remote_code=True,
                cache_dir=self.hf_cache_dir,
            )

        # Load model (Qwen3VLForConditionalGeneration)
        try:
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.hf_hub_path,
                trust_remote_code=True,
                local_files_only=True,
                cache_dir=self.hf_cache_dir,
                torch_dtype=(
                    torch.bfloat16 if enable_mixed_precision_training else torch.float32
                ),
            )
            overwatch.info(
                f"Qwen3VLForConditionalGeneration loaded from local cache", ctx_level=1
            )
        except Exception as e:
            overwatch.warning(
                f"Failed to load model from local cache: {e}. Attempting online download..."
            )
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.hf_hub_path,
                trust_remote_code=True,
                cache_dir=self.hf_cache_dir,
                torch_dtype=(
                    torch.bfloat16 if enable_mixed_precision_training else torch.float32
                ),
            )

        # é…ç½® tokenizer paddingï¼ˆæ”¹ä¸º left paddingï¼‰
        self.processor.tokenizer.padding_side = "right"
        if self.processor.tokenizer.pad_token is None:
            self.processor.tokenizer.pad_token = self.processor.tokenizer.eos_token
            self.model.config.pad_token_id = self.processor.tokenizer.eos_token_id

        overwatch.info(
            f"Tokenizer padding side: {self.processor.tokenizer.padding_side}",
            ctx_level=1,
        )

        # Module keys for checkpoint saving
        self.all_module_keys = ["model", "processor"]
        self.trainable_module_keys = []  # æ ¹æ® freeze_backbones åŠ¨æ€è®¾ç½®

        # === GenerationMixin Expected Attributes ===
        self.generation_config = self.model.generation_config
        self.main_input_name = "input_ids"

        overwatch.info(f"Qwen3-VL {model_size} initialized successfully", ctx_level=1)

    @property
    def device(self) -> torch.device:
        """Get model device."""
        return next(self.model.parameters()).device

    @property
    def config(self) -> PretrainedConfig:
        """Expose model config for GenerationMixin."""
        return self.model.config

    @classmethod
    def from_pretrained(
        cls,
        pretrained_checkpoint: Path,
        model_id: str,
        model_size: str = "2B",
        trajectory_converter: Optional[BaseTrajectoryConverter] = None,
        **kwargs,
    ) -> Qwen3VLA:
        """
        Load from checkpoint (for fine-tuned weights).

        Note: Base Qwen3-VL weights are always loaded from HF cache first,
        then checkpoint weights are loaded on top.
        """
        # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
        vla = cls(
            model_id=model_id,
            model_size=model_size,
            trajectory_converter=trajectory_converter,
            **kwargs,
        )

        # åŠ è½½ checkpoint æƒé‡
        if pretrained_checkpoint is not None and pretrained_checkpoint.exists():
            overwatch.info(f"Loading checkpoint from {pretrained_checkpoint}")
            from safetensors import safe_open

            with safe_open(
                str(pretrained_checkpoint), framework="pt", device="cpu"
            ) as f:
                state_dict = {k: f.get_tensor(k) for k in f.keys()}

            # è¿‡æ»¤å‡º model ç›¸å…³çš„æƒé‡
            model_state = {}
            for key, tensor in state_dict.items():
                if key.startswith("model."):
                    model_state[key[6:]] = tensor  # å»æ‰ "model." å‰ç¼€

            if model_state:
                vla.model.load_state_dict(model_state, strict=False)
                overwatch.info("Checkpoint weights loaded successfully")

        return vla

    def get_prompt_builder(self, system_prompt: Optional[str] = None) -> PromptBuilder:
        """
        Qwen3-VL ä½¿ç”¨è‡ªå·±çš„ chat templateï¼Œä¸éœ€è¦ PromptBuilderã€‚
        è¿™ä¸ªæ–¹æ³•ä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹ VLM æ¥å£ã€‚
        """
        from core.models.backbones.llm.prompting import PurePromptBuilder

        return PurePromptBuilder("qwen3-vl", system_prompt=system_prompt)

    def freeze_backbones(self, stage: str) -> None:
        """
        å†»ç»“éƒ¨åˆ†æ¨¡å‹å‚æ•°ã€‚

        Qwen3-VL çš„æ¶æ„ï¼š
        - model.visual: è§†è§‰ç¼–ç å™¨
        - model.model.layers: Transformer å±‚
        - model.lm_head: è¯­è¨€æ¨¡å‹å¤´

        æ”¯æŒçš„ stageï¼š
        - "vla-train": å†»ç»“è§†è§‰ç¼–ç å™¨ï¼Œè®­ç»ƒè¯­è¨€éƒ¨åˆ†
        - "vla-full-train": å…¨éƒ¨è®­ç»ƒ
        - "vla-last-layer-train": åªè®­ç»ƒæœ€åä¸€å±‚
        """
        if stage == "vla-train":
            # å†»ç»“è§†è§‰ç¼–ç å™¨
            if hasattr(self.model, "visual"):
                self.model.visual.requires_grad_(False)
                overwatch.info("[Frozen] ğŸ¥¶ =>> Visual Encoder", ctx_level=1)

            # è®­ç»ƒè¯­è¨€æ¨¡å‹éƒ¨åˆ†
            if hasattr(self.model, "model"):
                self.model.model.requires_grad_(True)
            if hasattr(self.model, "lm_head"):
                self.model.lm_head.requires_grad_(True)
            overwatch.info("[TRAINABLE] ğŸ”¥ =>> Language Model", ctx_level=1)

            self.trainable_module_keys = ["model"]

        elif stage == "vla-full-train":
            # å…¨éƒ¨è®­ç»ƒ
            self.model.requires_grad_(True)
            overwatch.info(
                "[TRAINABLE] ğŸ”¥ =>> Full Model (Vision + Language)", ctx_level=1
            )
            self.trainable_module_keys = ["model"]

        elif stage == "vla-last-layer-train":
            # åªè®­ç»ƒæœ€åä¸€å±‚
            self.model.requires_grad_(False)
            if hasattr(self.model, "model") and hasattr(self.model.model, "layers"):
                self.model.model.layers[-1].requires_grad_(True)
            if hasattr(self.model, "lm_head"):
                self.model.lm_head.requires_grad_(True)
            overwatch.info(
                "[Frozen, except last layer] ğŸ¥¶ğŸ”¥ =>> Language Model", ctx_level=1
            )
            self.trainable_module_keys = ["model"]

        else:
            raise ValueError(f"Unknown stage `{stage}` for Qwen3VLA")

    def load_from_checkpoint(
        self, stage: str, run_dir: Path, pretrained_checkpoint: Optional[Path] = None
    ) -> None:
        """Load checkpoint weights (compatibility method)."""
        if pretrained_checkpoint is not None and pretrained_checkpoint.exists():
            overwatch.info(f"Loading checkpoint: {pretrained_checkpoint}")
            # Implementation similar to from_pretrained
            pass

    def get_fsdp_wrapping_policy(self) -> Callable:
        """
        è¿”å› FSDP wrapping policyã€‚

        æ³¨æ„ï¼šQwen3-VL å¯èƒ½æ›´é€‚åˆç”¨ Accelerate + DeepSpeedï¼Œ
        ä½†è¿™é‡Œæä¾› FSDP ç­–ç•¥ä»¥å…¼å®¹æ¡†æ¶ã€‚
        """
        from functools import partial
        from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy

        # Qwen3 ä½¿ç”¨æ ‡å‡† Transformer block
        # éœ€è¦æ‰¾åˆ°å¯¹åº”çš„ layer class
        try:
            from transformers.models.qwen2_vl.modeling_qwen2_vl import (
                Qwen2VLDecoderLayer,
            )

            transformer_layer_cls = {Qwen2VLDecoderLayer}
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨ç­–ç•¥
            overwatch.warning(
                "Could not import Qwen2VLDecoderLayer, using default policy"
            )
            transformer_layer_cls = set()

        return partial(
            transformer_auto_wrap_policy,
            transformer_layer_cls=transformer_layer_cls,
        )

    def _prepare_inputs_for_qwen(
        self,
        pixel_values: Dict[str, torch.Tensor],
        input_ids: torch.LongTensor,
        attention_mask: torch.Tensor,
    ) -> Dict:
        """
        å‡†å¤‡ Qwen3-VL çš„è¾“å…¥æ ¼å¼ã€‚

        å¤„ç†æµç¨‹ï¼š
        1. ä» pixel_values dict ä¸­æå– cam1, cam2
        2. è½¬æ¢ä¸º PIL Image (processor æœŸæœ› PIL æ ¼å¼)
        3. æ„å»ºåŒ…å«å›¾åƒçš„ messages
        4. ä½¿ç”¨ processor å¤„ç†
        5. è¿½åŠ åŠ¨ä½œ tokens (input_ids)

        Args:
            pixel_values: {"cam1": [B, 3, H, W], "cam2": [B, 3, H, W]}
            input_ids: [B, seq_len] - åŒ…å« prompt å’Œ action tokens
            attention_mask: [B, seq_len]

        Returns:
            Dict with keys: input_ids, attention_mask, pixel_values, image_grid_thw
        """
        batch_size = input_ids.shape[0]
        device = input_ids.device

        # å‡†å¤‡å›¾åƒåˆ—è¡¨ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸€ä¸ªåˆ—è¡¨ï¼‰
        # Qwen3-VL processor å¯ä»¥å¤„ç†å¤šå›¾è¾“å…¥
        images_per_sample = []
        for b in range(batch_size):
            sample_images = []
            for cam_key in sorted(pixel_values.keys()):  # cam1, cam2
                img_tensor = pixel_values[cam_key][b]  # [3, H, W]

                # è½¬æ¢ä¸º PIL Image
                # æ³¨æ„ï¼šdataset è¾“å‡ºæ˜¯ [0, 1] èŒƒå›´çš„ tensor
                img_np = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(
                    np.uint8
                )
                pil_img = Image.fromarray(img_np)
                sample_images.append(pil_img)

            images_per_sample.append(sample_images)

        # ç°åœ¨æˆ‘ä»¬æœ‰äº†å›¾åƒï¼Œä½†è¿˜éœ€è¦æ„å»º text prompt
        # è¿™é‡Œçš„ input_ids å·²ç»åŒ…å«äº†å®Œæ•´çš„åºåˆ—ï¼ˆprompt + action tokensï¼‰
        # ä½† Qwen processor éœ€è¦çš„æ˜¯ messages æ ¼å¼

        # è§£å†³æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨ processor çš„ tokenizer decode å‡º textï¼Œ
        # ç„¶åé‡æ–°ç”¨ processor å¤„ç†ï¼ˆåŒ…æ‹¬å›¾åƒï¼‰

        # æ›´ç®€å•çš„æ–¹æ¡ˆï¼šæˆ‘ä»¬è·³è¿‡ processor çš„å®Œæ•´å¤„ç†ï¼Œ
        # åªç”¨å®ƒçš„ image processor å¤„ç†å›¾åƒï¼Œ
        # ç„¶åæ‰‹åŠ¨æ„å»ºæ¨¡å‹è¾“å…¥

        # ä½¿ç”¨ processor å¤„ç†å›¾åƒ
        # processor.image_processor å¯ä»¥æ‰¹é‡å¤„ç†
        all_images = []
        for sample_imgs in images_per_sample:
            all_images.extend(sample_imgs)

        # å¤„ç†å›¾åƒ
        image_inputs = self.processor.image_processor(
            images=all_images,
            return_tensors="pt",
        )

        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        for key in image_inputs:
            if isinstance(image_inputs[key], torch.Tensor):
                image_inputs[key] = image_inputs[key].to(device)

        # è¿”å›æ¨¡å‹æ‰€éœ€çš„è¾“å…¥
        # Qwen3-VL çš„ forward éœ€è¦ï¼š
        # - input_ids
        # - attention_mask
        # - pixel_values (from image_processor)
        # - image_grid_thw (from image_processor)

        model_inputs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
        }

        # æ·»åŠ å›¾åƒç›¸å…³çš„è¾“å…¥
        if "pixel_values" in image_inputs:
            model_inputs["pixel_values"] = image_inputs["pixel_values"]
        if "image_grid_thw" in image_inputs:
            model_inputs["image_grid_thw"] = image_inputs["image_grid_thw"]

        return model_inputs

    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        pixel_values: Optional[
            Union[torch.FloatTensor, Dict[str, torch.FloatTensor]]
        ] = None,
        labels: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> CausalLMOutputWithPast:
        """
        Qwen3-VL forward pass for VLA training.

        æ³¨æ„ï¼šinput_ids å’Œ labels å·²ç»ç”± VlaTokenizer å¤„ç†å¥½ï¼Œ
        åŒ…å«äº† prompt tokens å’Œ action tokensã€‚
        æˆ‘ä»¬åªéœ€è¦å¤„ç†å›¾åƒè¾“å…¥ã€‚
        """
        # å¦‚æœæœ‰ç¼“å­˜çš„ past_key_valuesï¼Œè¯´æ˜æ˜¯ generation çš„åç»­æ­¥éª¤
        # æ­¤æ—¶ä¸éœ€è¦å›¾åƒ
        if past_key_values is not None and pixel_values is None:
            return self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                labels=labels,
            )

        # é¦–æ¬¡å‰å‘ä¼ æ’­ï¼šéœ€è¦å¤„ç†å›¾åƒ
        if pixel_values is not None and isinstance(pixel_values, dict):
            # å‡†å¤‡ Qwen3-VL æ ¼å¼çš„è¾“å…¥
            model_inputs = self._prepare_inputs_for_qwen(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
            )

            # æ·»åŠ å…¶ä»–å‚æ•°
            model_inputs.update(
                {
                    "labels": labels,
                    "past_key_values": past_key_values,
                    "use_cache": use_cache,
                    "output_attentions": output_attentions,
                    "output_hidden_states": output_hidden_states,
                    "return_dict": return_dict,
                }
            )

            # è°ƒç”¨æ¨¡å‹
            return self.model(**model_inputs)
        else:
            # æ²¡æœ‰å›¾åƒï¼Œç›´æ¥å‰å‘ä¼ æ’­ï¼ˆçº¯æ–‡æœ¬æˆ–åç»­ç”Ÿæˆæ­¥éª¤ï¼‰
            return self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values,
                labels=labels,
                past_key_values=past_key_values,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
            )

    @torch.inference_mode()
    def generate_ids(
        self,
        image: Union[Image.Image, Dict[str, Image.Image]],
        prompt_text: str,
        **kwargs,
    ) -> torch.LongTensor:
        """
        ç”Ÿæˆ token IDsï¼ˆç”¨äºæ¨ç†ï¼‰ã€‚

        Args:
            image: PIL Image æˆ– {"cam1": PIL Image, "cam2": PIL Image}
            prompt_text: æ–‡æœ¬ prompt
            **kwargs: ä¼ é€’ç»™ generate çš„å‚æ•°

        Returns:
            ç”Ÿæˆçš„ token IDs [1, seq_len]
        """
        # å‡†å¤‡å›¾åƒåˆ—è¡¨
        if isinstance(image, dict):
            images = [image[k] for k in sorted(image.keys())]
        else:
            images = [image]

        # æ„å»º messagesï¼ˆQwen3-VL æ ¼å¼ï¼‰
        messages = [
            {
                "role": "user",
                "content": [{"type": "image", "image": img} for img in images]
                + [{"type": "text", "text": prompt_text}],
            }
        ]

        # ä½¿ç”¨ processor å‡†å¤‡è¾“å…¥
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        inputs = self.processor(
            text=[text],
            images=images,
            return_tensors="pt",
            padding=True,
        ).to(self.device)

        # ç”Ÿæˆ
        generated_ids = self.model.generate(**inputs, **kwargs)

        return generated_ids

    # === GenerationMixin Required Methods ===
    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwargs):
        """Prepare inputs for generation step."""
        return self.model.prepare_inputs_for_generation(
            input_ids, past_key_values=past_key_values, **kwargs
        )

    def _reorder_cache(self, past_key_values, beam_idx):
        """Reorder cache for beam search."""
        return self.model._reorder_cache(past_key_values, beam_idx)

    @property
    def llm_backbone(self):
        """
        å…¼å®¹å±æ€§ï¼šè¿”å›ä¸€ä¸ªåŒ…å« tokenizer å’Œ prompt_builder_fn çš„å¯¹è±¡ã€‚
        ç”¨äºä¸ç°æœ‰çš„ VLA è®­ç»ƒæµç¨‹å…¼å®¹ã€‚
        """

        class FakeLLMBackbone:
            def __init__(self, processor, model):
                self.processor = processor
                self.model = model
                self.tokenizer = processor.tokenizer

            def get_tokenizer(self):
                return self.tokenizer

            @property
            def prompt_builder_fn(self):
                from core.models.backbones.llm.prompting import PurePromptBuilder

                return PurePromptBuilder

            @property
            def transformer_layer_cls(self):
                # è¿”å› Qwen3 çš„ transformer layer class
                try:
                    from transformers.models.qwen2_vl.modeling_qwen2_vl import (
                        Qwen2VLDecoderLayer,
                    )

                    return Qwen2VLDecoderLayer
                except ImportError:
                    return nn.Module

            @property
            def last_layer_finetune_modules(self):
                # è¿”å›æœ€åä¸€å±‚çš„æ¨¡å—ï¼ˆç”¨äºéƒ¨åˆ†å¾®è°ƒï¼‰
                if hasattr(self.model, "model") and hasattr(self.model.model, "layers"):
                    return [self.model.model.layers[-1], self.model.lm_head]
                return []

        return FakeLLMBackbone(self.processor, self.model)

    @property
    def vision_backbone(self):
        """
        å…¼å®¹å±æ€§ï¼šè¿”å›ä¸€ä¸ªåŒ…å« image_transform çš„å¯¹è±¡ã€‚
        ç”¨äºä¸ç°æœ‰çš„ VLA è®­ç»ƒæµç¨‹å…¼å®¹ã€‚
        """

        class FakeVisionBackbone:
            def __init__(self, processor):
                self.processor = processor
                self.identifier = "qwen3-vl-vision"

            def get_image_transform(self):
                """
                è¿”å›å›¾åƒè½¬æ¢å™¨ã€‚

                é‡è¦ï¼šè¿™ä¸ª transform ä¼šè¢«ä¼ é€’ç»™ LeRobotDatasetã€‚
                LeRobotDataset è¾“å‡ºçš„å›¾åƒæ˜¯ [0, 1] èŒƒå›´çš„ tensorã€‚
                æˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸º Qwen processor å¯ä»¥å¤„ç†çš„æ ¼å¼ã€‚
                """
                return Qwen3ImageTransform(self.processor)

            def get_fsdp_wrapping_policy(self):
                """ç©ºç­–ç•¥ï¼ˆQwen3-VL ä¸éœ€è¦å•ç‹¬çš„ vision wrappingï¼‰"""
                from functools import partial
                from torch.distributed.fsdp.wrap import _module_wrap_policy

                return partial(_module_wrap_policy, module_classes=set())

        return FakeVisionBackbone(self.processor)


class Qwen3ImageTransform:
    """
    Qwen3-VL çš„å›¾åƒè½¬æ¢å™¨ã€‚

    è´Ÿè´£å°† LeRobotDataset è¾“å‡ºçš„ [0, 1] tensor è½¬æ¢ä¸º
    Qwen processor å¯ä»¥å¤„ç†çš„æ ¼å¼ã€‚

    æ³¨æ„ï¼šLeRobotDataset ä¼šåº”ç”¨è¿™ä¸ª transformï¼Œ
    ä½† transform çš„è¾“å‡ºä»ç„¶æ˜¯ tensorï¼ˆè€Œä¸æ˜¯ PILï¼‰ï¼Œ
    å› ä¸ºæˆ‘ä»¬éœ€è¦åœ¨ batch collation æ—¶ä¿æŒ tensor æ ¼å¼ã€‚

    å®é™…çš„ PIL è½¬æ¢ä¼šåœ¨ forward() ä¸­è¿›è¡Œã€‚
    """

    def __init__(self, processor):
        self.processor = processor

    def __call__(self, img: Union[Image.Image, torch.Tensor]) -> torch.Tensor:
        """
        è½¬æ¢å›¾åƒã€‚

        Args:
            img: PIL Image æˆ– torch.Tensor [C, H, W] in [0, 1]

        Returns:
            torch.Tensor [C, H, W] in [0, 1] (ä¿æŒåŸæ ·ï¼Œå®é™…è½¬æ¢åœ¨ forward ä¸­)
        """
        # LeRobotDataset å¯èƒ½ä¼ å…¥ PIL æˆ– tensor
        if isinstance(img, Image.Image):
            # è½¬æ¢ä¸º tensor [0, 1]
            import torchvision.transforms.functional as TF

            return TF.to_tensor(img)
        elif isinstance(img, torch.Tensor):
            # å·²ç»æ˜¯ tensorï¼Œç›´æ¥è¿”å›
            # ç¡®ä¿æ˜¯ [C, H, W] æ ¼å¼
            if img.ndim == 4:  # [B, C, H, W]
                img = img[0]
            return img
        else:
            raise TypeError(f"Unexpected image type: {type(img)}")
